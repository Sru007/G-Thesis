{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1c8777-21bd-4557-b6cc-022b8ed9f940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bdbb90e-31f7-4a56-86f7-62f9c7455c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init('/opt/spark/spark-3.4.4-bin-hadoop3')  # Set to the Spark root directory\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"IntrusionDetection\") \\\n",
    "#     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fde7a8-ea67-482e-9bbb-2ee724e525be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/10 20:03:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark/spark-3.4.4-bin-hadoop3')  # Adjust to your Spark root directory\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with advanced configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IntrusionDetection\") \\\n",
    "    .config(\"spark.master\", \"spark://10.154.0.2:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc06587f-4151-4cb0-bf6b-9e22e537ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset in HDFS\n",
    "hdfs_path = \"hdfs://localhost:54310/datasets/NF-UQ-NIDS-v2.csv\"  # Adjust the filename if necessary\n",
    "# Define the path to the dataset in HDFS\n",
    "#hdfs_path = \"hdfs:///datasets/NF-UQ-NIDS-v2/dataset.csv\"  # Adjust the filename if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a6f774-97df-4fb2-aaf4-6ee7073cdadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e077233-23f8-4e9c-8d08-4ffce0c0a756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IPV4_SRC_ADDR: string (nullable = true)\n",
      " |-- L4_SRC_PORT: integer (nullable = true)\n",
      " |-- IPV4_DST_ADDR: string (nullable = true)\n",
      " |-- L4_DST_PORT: integer (nullable = true)\n",
      " |-- PROTOCOL: integer (nullable = true)\n",
      " |-- L7_PROTO: double (nullable = true)\n",
      " |-- IN_BYTES: integer (nullable = true)\n",
      " |-- IN_PKTS: integer (nullable = true)\n",
      " |-- OUT_BYTES: integer (nullable = true)\n",
      " |-- OUT_PKTS: integer (nullable = true)\n",
      " |-- TCP_FLAGS: integer (nullable = true)\n",
      " |-- CLIENT_TCP_FLAGS: integer (nullable = true)\n",
      " |-- SERVER_TCP_FLAGS: integer (nullable = true)\n",
      " |-- FLOW_DURATION_MILLISECONDS: integer (nullable = true)\n",
      " |-- DURATION_IN: integer (nullable = true)\n",
      " |-- DURATION_OUT: integer (nullable = true)\n",
      " |-- MIN_TTL: integer (nullable = true)\n",
      " |-- MAX_TTL: integer (nullable = true)\n",
      " |-- LONGEST_FLOW_PKT: integer (nullable = true)\n",
      " |-- SHORTEST_FLOW_PKT: integer (nullable = true)\n",
      " |-- MIN_IP_PKT_LEN: integer (nullable = true)\n",
      " |-- MAX_IP_PKT_LEN: integer (nullable = true)\n",
      " |-- SRC_TO_DST_SECOND_BYTES: double (nullable = true)\n",
      " |-- DST_TO_SRC_SECOND_BYTES: double (nullable = true)\n",
      " |-- RETRANSMITTED_IN_BYTES: integer (nullable = true)\n",
      " |-- RETRANSMITTED_IN_PKTS: integer (nullable = true)\n",
      " |-- RETRANSMITTED_OUT_BYTES: integer (nullable = true)\n",
      " |-- RETRANSMITTED_OUT_PKTS: integer (nullable = true)\n",
      " |-- SRC_TO_DST_AVG_THROUGHPUT: long (nullable = true)\n",
      " |-- DST_TO_SRC_AVG_THROUGHPUT: long (nullable = true)\n",
      " |-- NUM_PKTS_UP_TO_128_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_128_TO_256_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_256_TO_512_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_512_TO_1024_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_1024_TO_1514_BYTES: integer (nullable = true)\n",
      " |-- TCP_WIN_MAX_IN: integer (nullable = true)\n",
      " |-- TCP_WIN_MAX_OUT: integer (nullable = true)\n",
      " |-- ICMP_TYPE: integer (nullable = true)\n",
      " |-- ICMP_IPV4_TYPE: integer (nullable = true)\n",
      " |-- DNS_QUERY_ID: integer (nullable = true)\n",
      " |-- DNS_QUERY_TYPE: integer (nullable = true)\n",
      " |-- DNS_TTL_ANSWER: long (nullable = true)\n",
      " |-- FTP_COMMAND_RET_CODE: double (nullable = true)\n",
      " |-- Label: integer (nullable = true)\n",
      " |-- Attack: string (nullable = true)\n",
      " |-- Dataset: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 19:58:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------+-------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+--------------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+--------------------+------------------+--------+---------------+\n",
      "|summary|IPV4_SRC_ADDR|       L4_SRC_PORT|IPV4_DST_ADDR|      L4_DST_PORT|          PROTOCOL|         L7_PROTO|          IN_BYTES|          IN_PKTS|         OUT_BYTES|         OUT_PKTS|         TCP_FLAGS|  CLIENT_TCP_FLAGS|  SERVER_TCP_FLAGS|FLOW_DURATION_MILLISECONDS|      DURATION_IN|      DURATION_OUT|           MIN_TTL|          MAX_TTL|  LONGEST_FLOW_PKT|SHORTEST_FLOW_PKT|    MIN_IP_PKT_LEN|    MAX_IP_PKT_LEN|SRC_TO_DST_SECOND_BYTES|DST_TO_SRC_SECOND_BYTES|RETRANSMITTED_IN_BYTES|RETRANSMITTED_IN_PKTS|RETRANSMITTED_OUT_BYTES|RETRANSMITTED_OUT_PKTS|SRC_TO_DST_AVG_THROUGHPUT|DST_TO_SRC_AVG_THROUGHPUT|NUM_PKTS_UP_TO_128_BYTES|NUM_PKTS_128_TO_256_BYTES|NUM_PKTS_256_TO_512_BYTES|NUM_PKTS_512_TO_1024_BYTES|NUM_PKTS_1024_TO_1514_BYTES|    TCP_WIN_MAX_IN|   TCP_WIN_MAX_OUT|        ICMP_TYPE|   ICMP_IPV4_TYPE|      DNS_QUERY_ID|    DNS_QUERY_TYPE|    DNS_TTL_ANSWER|FTP_COMMAND_RET_CODE|             Label|  Attack|        Dataset|\n",
      "+-------+-------------+------------------+-------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+--------------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+--------------------+------------------+--------+---------------+\n",
      "|  count|     75987976|          75987976|     75987976|         75987976|          75987976|         75987976|          75987976|         75987976|          75987976|         75987976|          75987976|          75987976|          75987976|                  75987976|         75987976|          75987976|          75987976|         75987976|          75987976|         75987976|          75987976|          75987976|               75987976|               75987976|              75987976|             75987976|               75987976|              75987976|                 75987976|                 75987976|                75987976|                 75987976|                 75987976|                  75987976|                   75987976|          75987976|          75987976|         75987976|         75987976|          75987976|          75987976|          75987976|            75987976|          75987976|75987976|       75987976|\n",
      "|   mean|         null|40398.338208889785|         null|3336.835107517537|10.247083578065036|53.31183304869686|1029.5340386221105|9.662047571842155|3116.2212573868264|4.722149198973269| 24.16826992470493|22.164622071260325|  9.51846613206279|         2315190.587616954|533.4617826904614|14.830757618810638|53.593917950913706| 53.6517743280858|264.86331162446015| 62.7776641004361|23.582000894457302|264.86331162446015|   2.040588355953292...|   2.463793714826146...|     78.43725227001704|  0.39389230475095166|      568.3953973586558|    0.5726637330095488|       2847345.4971513916|        7205773.168111965|       29.09634600611023|       0.9676467497963098|      0.36456773108419155|        0.3700401100300395|          2.019031313585718| 6006.523034539044| 8382.782375095765|3749.752313852918|14.64745801362047| 4524.422103899701|1.4077446673931675|19870.159318508497|    1.51512914358977|0.6688253020451551|    null|           null|\n",
      "| stddev|         null| 18413.35975094483|         null| 10659.9639333031| 5.641418228180131|78.90932610522903|107346.44691982033|599.7798198306356|263347.41634031665|213.0038495893294|58.433996193338196| 58.51013788137136|18.037666097628037|         2140388.092283884|847.5262982165216|113.78847031309567| 39.55919160753938|39.59975804561248| 430.3351361199158|44.82738569100848|26.981311745765126| 430.3351361199158|                    NaN|               Infinity|     6983.825477773818|   11.630251415944326|     14447.401646059456|    10.380102460007848|      1.570953059153987E7|     6.6195318588321224E7|      1601.1786183666031|       17.134716086908238|       17.371948540812294|        49.040027872599445|          179.5986451266384|13218.279048601162|17704.126865844428|11535.69724788917|45.06129156210396|13313.891760014076|23.746322554080457| 8955740.302768406|  20.389193502724687|0.4706357618204035|    null|           null|\n",
      "|    min|      0.0.0.0|                 0|      0.0.0.0|                0|                 0|              0.0|                 1|                1|                 0|                0|                 0|                 0|                 0|                         0|                0|                 0|                 0|                0|                28|               28|                 0|                28|                    0.0|                    0.0|                     0|                    0|                      0|                     0|                        0|                        0|                       0|                        0|                        0|                         0|                          0|                 0|                 0|                0|                0|                 0|                 0|                 0|                 0.0|                 0|Analysis|  NF-BoT-IoT-v2|\n",
      "|    max| 99.99.45.194|             65535| 99.99.99.142|            65535|               255|            248.0|         301926201|           469281|         275639781|           410903|               223|               223|               223|                   4294967|           119574|            119558|               255|              255|             65535|             3990|              1260|             65535|   8.91211152106772E304|   1.87216383510445E257|              15120489|                13796|                7329139|                  5536|               4277408000|               4294480000|                  475657|                    73465|                    47667|                     38812|                     200984|             65535|             65535|            65317|              255|             65535|             55937|        4294915672|               553.0|                 1|     xss|NF-UNSW-NB15-v2|\n",
      "+-------+-------------+------------------+-------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+--------------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+--------------------+------------------+--------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show summary statistics of the dataset\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e234c2-9641-4dc6-ad25-d230f20c0298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 14:42:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+-----------+--------+--------+--------+-------+---------+--------+---------+----------------+----------------+--------------------------+-----------+------------+-------+-------+----------------+-----------------+--------------+--------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+--------------+---------------+---------+--------------+------------+--------------+--------------+--------------------+-----+------+-------+\n",
      "|IPV4_SRC_ADDR|L4_SRC_PORT|IPV4_DST_ADDR|L4_DST_PORT|PROTOCOL|L7_PROTO|IN_BYTES|IN_PKTS|OUT_BYTES|OUT_PKTS|TCP_FLAGS|CLIENT_TCP_FLAGS|SERVER_TCP_FLAGS|FLOW_DURATION_MILLISECONDS|DURATION_IN|DURATION_OUT|MIN_TTL|MAX_TTL|LONGEST_FLOW_PKT|SHORTEST_FLOW_PKT|MIN_IP_PKT_LEN|MAX_IP_PKT_LEN|SRC_TO_DST_SECOND_BYTES|DST_TO_SRC_SECOND_BYTES|RETRANSMITTED_IN_BYTES|RETRANSMITTED_IN_PKTS|RETRANSMITTED_OUT_BYTES|RETRANSMITTED_OUT_PKTS|SRC_TO_DST_AVG_THROUGHPUT|DST_TO_SRC_AVG_THROUGHPUT|NUM_PKTS_UP_TO_128_BYTES|NUM_PKTS_128_TO_256_BYTES|NUM_PKTS_256_TO_512_BYTES|NUM_PKTS_512_TO_1024_BYTES|NUM_PKTS_1024_TO_1514_BYTES|TCP_WIN_MAX_IN|TCP_WIN_MAX_OUT|ICMP_TYPE|ICMP_IPV4_TYPE|DNS_QUERY_ID|DNS_QUERY_TYPE|DNS_TTL_ANSWER|FTP_COMMAND_RET_CODE|Label|Attack|Dataset|\n",
      "+-------------+-----------+-------------+-----------+--------+--------+--------+-------+---------+--------+---------+----------------+----------------+--------------------------+-----------+------------+-------+-------+----------------+-----------------+--------------+--------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+--------------+---------------+---------+--------------+------------+--------------+--------------+--------------------+-----+------+-------+\n",
      "|            0|          0|            0|          0|       0|       0|       0|      0|        0|       0|        0|               0|               0|                         0|          0|           0|      0|      0|               0|                0|             0|             0|                      0|                      0|                     0|                    0|                      0|                     0|                        0|                        0|                       0|                        0|                        0|                         0|                          0|             0|              0|        0|             0|           0|             0|             0|                   0|    0|     0|      0|\n",
      "+-------------+-----------+-------------+-----------+--------+--------+--------+-------+---------+--------+---------+----------------+----------------+--------------------------+-----------+------------+-------+-------+----------------+-----------------+--------------+--------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+--------------+---------------+---------+--------------+------------+--------------+--------------+--------------------+-----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------------+-----------+--------+--------+--------+-------+---------+--------+---------+----------------+----------------+--------------------------+-----------+------------+-------+-------+----------------+-----------------+--------------+--------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+--------------+---------------+---------+--------------+------------+--------------+--------------+--------------------+-----+------+-------+\n",
      "|IPV4_SRC_ADDR|L4_SRC_PORT|IPV4_DST_ADDR|L4_DST_PORT|PROTOCOL|L7_PROTO|IN_BYTES|IN_PKTS|OUT_BYTES|OUT_PKTS|TCP_FLAGS|CLIENT_TCP_FLAGS|SERVER_TCP_FLAGS|FLOW_DURATION_MILLISECONDS|DURATION_IN|DURATION_OUT|MIN_TTL|MAX_TTL|LONGEST_FLOW_PKT|SHORTEST_FLOW_PKT|MIN_IP_PKT_LEN|MAX_IP_PKT_LEN|SRC_TO_DST_SECOND_BYTES|DST_TO_SRC_SECOND_BYTES|RETRANSMITTED_IN_BYTES|RETRANSMITTED_IN_PKTS|RETRANSMITTED_OUT_BYTES|RETRANSMITTED_OUT_PKTS|SRC_TO_DST_AVG_THROUGHPUT|DST_TO_SRC_AVG_THROUGHPUT|NUM_PKTS_UP_TO_128_BYTES|NUM_PKTS_128_TO_256_BYTES|NUM_PKTS_256_TO_512_BYTES|NUM_PKTS_512_TO_1024_BYTES|NUM_PKTS_1024_TO_1514_BYTES|TCP_WIN_MAX_IN|TCP_WIN_MAX_OUT|ICMP_TYPE|ICMP_IPV4_TYPE|DNS_QUERY_ID|DNS_QUERY_TYPE|DNS_TTL_ANSWER|FTP_COMMAND_RET_CODE|Label|Attack|Dataset|\n",
      "+-------------+-----------+-------------+-----------+--------+--------+--------+-------+---------+--------+---------+----------------+----------------+--------------------------+-----------+------------+-------+-------+----------------+-----------------+--------------+--------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+--------------+---------------+---------+--------------+------------+--------------+--------------+--------------------+-----+------+-------+\n",
      "|       247700|      65536|        40529|      65536|     256|     375|   57309|  11233|   141218|    5579|       53|              51|              44|                      5225|       5209|        2531|    191|    190|            2040|             1235|           228|          2040|                 336053|                 247825|                 13386|                  794|                  18944|                   793|                    27868|                   106036|                   25827|                     2260|                     1456|                      1998|                       5035|         54852|          13895|     1975|           256|       65536|            27|         13003|                  21|    2|    21|      4|\n",
      "+-------------+-----------+-------------+-----------+--------+--------+--------+-------+---------+--------+---------+----------------+----------------+--------------------------+-----------+------------+-------+-------+----------------+-----------------+--------------+--------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+--------------+---------------+---------+--------------+------------+--------------+--------------+--------------------+-----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import necessary functions from PySpark\n",
    "from pyspark.sql.functions import col, sum as spark_sum, countDistinct\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = df.select([(spark_sum(col(c).isNull().cast(\"int\")).alias(c)) for c in df.columns])\n",
    "missing_values.show()\n",
    "\n",
    "# Get the unique count of values for each column\n",
    "unique_counts = df.agg(*[countDistinct(col(c)).alias(c) for c in df.columns])\n",
    "unique_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e4c175-ed0c-4a4c-8b8e-7f82afe14611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/09 09:34:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+-----------------+------------------+-----------------+--------------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+------------------+------------------+--------------------+\n",
      "|summary|       L4_SRC_PORT|      L4_DST_PORT|          IN_BYTES|          IN_PKTS|         OUT_BYTES|         OUT_PKTS|FLOW_DURATION_MILLISECONDS|      DURATION_IN|      DURATION_OUT|           MIN_TTL|          MAX_TTL|  LONGEST_FLOW_PKT|SHORTEST_FLOW_PKT|    MIN_IP_PKT_LEN|    MAX_IP_PKT_LEN|SRC_TO_DST_SECOND_BYTES|DST_TO_SRC_SECOND_BYTES|RETRANSMITTED_IN_BYTES|RETRANSMITTED_IN_PKTS|RETRANSMITTED_OUT_BYTES|RETRANSMITTED_OUT_PKTS|SRC_TO_DST_AVG_THROUGHPUT|DST_TO_SRC_AVG_THROUGHPUT|NUM_PKTS_UP_TO_128_BYTES|NUM_PKTS_128_TO_256_BYTES|NUM_PKTS_256_TO_512_BYTES|NUM_PKTS_512_TO_1024_BYTES|NUM_PKTS_1024_TO_1514_BYTES|    TCP_WIN_MAX_IN|   TCP_WIN_MAX_OUT|FTP_COMMAND_RET_CODE|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+-----------------+--------------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+------------------+------------------+--------------------+\n",
      "|  count|          75987976|         75987976|          75987976|         75987976|          75987976|         75987976|                  75987976|         75987976|          75987976|          75987976|         75987976|          75987976|         75987976|          75987976|          75987976|               75987976|               75987976|              75987976|             75987976|               75987976|              75987976|                 75987976|                 75987976|                75987976|                 75987976|                 75987976|                  75987976|                   75987976|          75987976|          75987976|            75987976|\n",
      "|   mean|40398.338208889785|3336.835107517537|1029.5340386221105|9.662047571842155|3116.2212573868264|4.722149198973269|         2315190.587616954|533.4617826904614|14.830757618810638|53.593917950913706| 53.6517743280858|264.86331162446015| 62.7776641004361|23.582000894457302|264.86331162446015|   2.040588355953292...|   2.463793714826146...|     78.43725227001704|  0.39389230475095166|      568.3953973586558|    0.5726637330095488|       2847345.4971513916|        7205773.168111965|       29.09634600611023|       0.9676467497963098|      0.36456773108419155|        0.3700401100300395|          2.019031313585718| 6006.523034539044| 8382.782375095765|    1.51512914358977|\n",
      "| stddev|18413.359750944826| 10659.9639333031|107346.44691982032|599.7798198306356|263347.41634031676|213.0038495893294|         2140388.092283884|847.5262982165211|113.78847031309566|39.559191607539375|39.59975804561248| 430.3351361199158|44.82738569100847|26.981311745765126| 430.3351361199158|                    NaN|               Infinity|    6983.8254777738175|   11.630251415944326|     14447.401646059456|    10.380102460007853|     1.5709530591539875E7|     6.6195318588321246E7|      1601.1786183666036|       17.134716086908234|         17.3719485408123|        49.040027872599445|          179.5986451266384|13218.279048601156|17704.126865844428|  20.389193502724687|\n",
      "|    min|                 0|                0|                 1|                1|                 0|                0|                         0|                0|                 0|                 0|                0|                28|               28|                 0|                28|                    0.0|                    0.0|                     0|                    0|                      0|                     0|                        0|                        0|                       0|                        0|                        0|                         0|                          0|                 0|                 0|                 0.0|\n",
      "|    max|             65535|            65535|         301926201|           469281|         275639781|           410903|                   4294967|           119574|            119558|               255|              255|             65535|             3990|              1260|             65535|   8.91211152106772E304|   1.87216383510445E257|              15120489|                13796|                7329139|                  5536|               4277408000|               4294480000|                  475657|                    73465|                    47667|                     38812|                     200984|             65535|             65535|               553.0|\n",
      "+-------+------------------+-----------------+------------------+-----------------+------------------+-----------------+--------------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------------+-----------------------+----------------------+---------------------+-----------------------+----------------------+-------------------------+-------------------------+------------------------+-------------------------+-------------------------+--------------------------+---------------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|PROTOCOL|\n",
      "+--------+\n",
      "|244     |\n",
      "|65      |\n",
      "|203     |\n",
      "|130     |\n",
      "|161     |\n",
      "|248     |\n",
      "|13      |\n",
      "|232     |\n",
      "|67      |\n",
      "|171     |\n",
      "|25      |\n",
      "|190     |\n",
      "|198     |\n",
      "|126     |\n",
      "|18      |\n",
      "|110     |\n",
      "|150     |\n",
      "|225     |\n",
      "|238     |\n",
      "|157     |\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TCP_FLAGS|\n",
      "+---------+\n",
      "|18       |\n",
      "|25       |\n",
      "|218      |\n",
      "|223      |\n",
      "|19       |\n",
      "|222      |\n",
      "|23       |\n",
      "|30       |\n",
      "|17       |\n",
      "|61       |\n",
      "|16       |\n",
      "|194      |\n",
      "|20       |\n",
      "|6        |\n",
      "|29       |\n",
      "|82       |\n",
      "|43       |\n",
      "|152      |\n",
      "|4        |\n",
      "|2        |\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|CLIENT_TCP_FLAGS|\n",
      "+----------------+\n",
      "|18              |\n",
      "|25              |\n",
      "|198             |\n",
      "|218             |\n",
      "|222             |\n",
      "|19              |\n",
      "|6               |\n",
      "|30              |\n",
      "|16              |\n",
      "|223             |\n",
      "|17              |\n",
      "|194             |\n",
      "|20              |\n",
      "|23              |\n",
      "|43              |\n",
      "|29              |\n",
      "|152             |\n",
      "|2               |\n",
      "|31              |\n",
      "|4               |\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|SERVER_TCP_FLAGS|\n",
      "+----------------+\n",
      "|18              |\n",
      "|25              |\n",
      "|95              |\n",
      "|218             |\n",
      "|83              |\n",
      "|157             |\n",
      "|20              |\n",
      "|19              |\n",
      "|94              |\n",
      "|16              |\n",
      "|30              |\n",
      "|17              |\n",
      "|6               |\n",
      "|82              |\n",
      "|222             |\n",
      "|86              |\n",
      "|23              |\n",
      "|29              |\n",
      "|223             |\n",
      "|31              |\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|ICMP_TYPE|\n",
      "+---------+\n",
      "|32000    |\n",
      "|771      |\n",
      "|6912     |\n",
      "|59392    |\n",
      "|11008    |\n",
      "|48128    |\n",
      "|41984    |\n",
      "|28672    |\n",
      "|48640    |\n",
      "|37120    |\n",
      "|3328     |\n",
      "|20992    |\n",
      "|36864    |\n",
      "|8960     |\n",
      "|53760    |\n",
      "|3072     |\n",
      "|27904    |\n",
      "|60416    |\n",
      "|18944    |\n",
      "|30720    |\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|ICMP_IPV4_TYPE|\n",
      "+--------------+\n",
      "|140           |\n",
      "|245           |\n",
      "|232           |\n",
      "|190           |\n",
      "|13            |\n",
      "|148           |\n",
      "|73            |\n",
      "|12            |\n",
      "|109           |\n",
      "|248           |\n",
      "|83            |\n",
      "|52            |\n",
      "|150           |\n",
      "|110           |\n",
      "|126           |\n",
      "|93            |\n",
      "|198           |\n",
      "|218           |\n",
      "|244           |\n",
      "|97            |\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|DNS_QUERY_ID|\n",
      "+------------+\n",
      "|25376       |\n",
      "|51839       |\n",
      "|52127       |\n",
      "|54862       |\n",
      "|62215       |\n",
      "|248         |\n",
      "|45824       |\n",
      "|27344       |\n",
      "|5634        |\n",
      "|10170       |\n",
      "|15044       |\n",
      "|18934       |\n",
      "|26853       |\n",
      "|39485       |\n",
      "|21360       |\n",
      "|58462       |\n",
      "|53972       |\n",
      "|21858       |\n",
      "|5942        |\n",
      "|2224        |\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|DNS_QUERY_TYPE|\n",
      "+--------------+\n",
      "|12            |\n",
      "|46            |\n",
      "|255           |\n",
      "|43            |\n",
      "|16            |\n",
      "|48            |\n",
      "|28            |\n",
      "|2             |\n",
      "|0             |\n",
      "|1             |\n",
      "|33            |\n",
      "|52            |\n",
      "|252           |\n",
      "|15            |\n",
      "|8465          |\n",
      "|251           |\n",
      "|32769         |\n",
      "|6             |\n",
      "|4096          |\n",
      "|55937         |\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Label|\n",
      "+-----+\n",
      "|1    |\n",
      "|0    |\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Attack        |\n",
      "+--------------+\n",
      "|Benign        |\n",
      "|DDoS          |\n",
      "|Fuzzers       |\n",
      "|ransomware    |\n",
      "|Reconnaissance|\n",
      "|injection     |\n",
      "|Exploits      |\n",
      "|DoS           |\n",
      "|Bot           |\n",
      "|Infilteration |\n",
      "|Generic       |\n",
      "|Analysis      |\n",
      "|mitm          |\n",
      "|Worms         |\n",
      "|scanning      |\n",
      "|xss           |\n",
      "|password      |\n",
      "|Theft         |\n",
      "|Brute Force   |\n",
      "|Backdoor      |\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Dataset              |\n",
      "+---------------------+\n",
      "|NF-BoT-IoT-v2        |\n",
      "|NF-ToN-IoT-v2        |\n",
      "|NF-CSE-CIC-IDS2018-v2|\n",
      "|NF-UNSW-NB15-v2      |\n",
      "+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define categorical and continuous features based on unique value counts and feature characteristics\n",
    "categorical_features = [\n",
    "    \"PROTOCOL\", \"TCP_FLAGS\", \"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \n",
    "    \"ICMP_TYPE\", \"ICMP_IPV4_TYPE\", \"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\", \n",
    "    \"Label\", \"Attack\", \"Dataset\"\n",
    "]\n",
    "\n",
    "continuous_features = [\n",
    "    \"L4_SRC_PORT\", \"L4_DST_PORT\", \"IN_BYTES\", \"IN_PKTS\", \"OUT_BYTES\", \n",
    "    \"OUT_PKTS\", \"FLOW_DURATION_MILLISECONDS\", \"DURATION_IN\", \"DURATION_OUT\", \n",
    "    \"MIN_TTL\", \"MAX_TTL\", \"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \n",
    "    \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"SRC_TO_DST_SECOND_BYTES\", \n",
    "    \"DST_TO_SRC_SECOND_BYTES\", \"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\", \n",
    "    \"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\", \"SRC_TO_DST_AVG_THROUGHPUT\", \n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\", \"NUM_PKTS_UP_TO_128_BYTES\", \"NUM_PKTS_128_TO_256_BYTES\", \n",
    "    \"NUM_PKTS_256_TO_512_BYTES\", \"NUM_PKTS_512_TO_1024_BYTES\", \"NUM_PKTS_1024_TO_1514_BYTES\", \n",
    "    \"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\", \"FTP_COMMAND_RET_CODE\"\n",
    "]\n",
    "\n",
    "# Show summary statistics for continuous features\n",
    "df.select([col(c) for c in continuous_features]).describe().show()\n",
    "\n",
    "# Show unique value counts for categorical features\n",
    "for feature in categorical_features:\n",
    "    df.select(feature).distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b9a670-8899-4f6c-814a-98969d29ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================================> (101 + 2) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------------+-----------------------+-------------------------+-------------------------+\n",
      "|summary|          IN_BYTES|         OUT_BYTES|SRC_TO_DST_SECOND_BYTES|DST_TO_SRC_SECOND_BYTES|SRC_TO_DST_AVG_THROUGHPUT|DST_TO_SRC_AVG_THROUGHPUT|\n",
      "+-------+------------------+------------------+-----------------------+-----------------------+-------------------------+-------------------------+\n",
      "|  count|          75987976|          75987976|               75987976|               75987976|                 75987976|                 75987976|\n",
      "|   mean| 684.9687731516891|1931.1599471605875|     53844.412021870405|      8083.790723982436|        2551493.682922677|        3930067.335627521|\n",
      "| stddev|10906.559260381766|28458.865621628727|     157701.53738160757|      79557.19427151275|        7246163.087712197|     1.3677411811359156E7|\n",
      "|    min|               1.0|               0.0|                    0.0|                    0.0|                      0.0|                      0.0|\n",
      "|    max|         1000000.0|         1000000.0|              1000000.0|              1000000.0|                    1.0E8|                    1.0E8|\n",
      "+-------+------------------+------------------+-----------------------+-----------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Define outlier thresholds based on domain knowledge or visual inspection\n",
    "outlier_thresholds = {\n",
    "    \"IN_BYTES\": 1e6,\n",
    "    \"OUT_BYTES\": 1e6,\n",
    "    \"SRC_TO_DST_SECOND_BYTES\": 1e6,\n",
    "    \"DST_TO_SRC_SECOND_BYTES\": 1e6,\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\": 1e8,\n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\": 1e8\n",
    "}\n",
    "\n",
    "# Cap outliers in specified columns\n",
    "for col_name, threshold in outlier_thresholds.items():\n",
    "    df = df.withColumn(\n",
    "        col_name, \n",
    "        when(col(col_name) > threshold, threshold).otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Verify the changes with describe\n",
    "df.select(list(outlier_thresholds.keys())).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dae9224-9282-4b2b-a436-ae29ccf496d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.24.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c10e5a-2e52-46ba-8ee2-f2b7f6670e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+-------------+\n",
      "|PROTOCOL_VEC   |TCP_FLAGS_VEC |Attack_VEC    |Dataset_VEC  |\n",
      "+---------------+--------------+--------------+-------------+\n",
      "|(255,[0],[1.0])|(52,[1],[1.0])|(20,[2],[1.0])|(3,[0],[1.0])|\n",
      "|(255,[0],[1.0])|(52,[2],[1.0])|(20,[2],[1.0])|(3,[0],[1.0])|\n",
      "|(255,[0],[1.0])|(52,[2],[1.0])|(20,[0],[1.0])|(3,[2],[1.0])|\n",
      "|(255,[0],[1.0])|(52,[2],[1.0])|(20,[0],[1.0])|(3,[2],[1.0])|\n",
      "|(255,[0],[1.0])|(52,[2],[1.0])|(20,[0],[1.0])|(3,[2],[1.0])|\n",
      "+---------------+--------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define StringIndexers for categorical features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"PROTOCOL\", outputCol=\"PROTOCOL_IDX\"),\n",
    "    StringIndexer(inputCol=\"TCP_FLAGS\", outputCol=\"TCP_FLAGS_IDX\"),\n",
    "    StringIndexer(inputCol=\"CLIENT_TCP_FLAGS\", outputCol=\"CLIENT_TCP_FLAGS_IDX\"),\n",
    "    StringIndexer(inputCol=\"SERVER_TCP_FLAGS\", outputCol=\"SERVER_TCP_FLAGS_IDX\"),\n",
    "    StringIndexer(inputCol=\"Attack\", outputCol=\"Attack_IDX\"),\n",
    "    StringIndexer(inputCol=\"Dataset\", outputCol=\"Dataset_IDX\")\n",
    "]\n",
    "\n",
    "# Define OneHotEncoders for indexed features\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=\"PROTOCOL_IDX\", outputCol=\"PROTOCOL_VEC\"),\n",
    "    OneHotEncoder(inputCol=\"TCP_FLAGS_IDX\", outputCol=\"TCP_FLAGS_VEC\"),\n",
    "    OneHotEncoder(inputCol=\"CLIENT_TCP_FLAGS_IDX\", outputCol=\"CLIENT_TCP_FLAGS_VEC\"),\n",
    "    OneHotEncoder(inputCol=\"SERVER_TCP_FLAGS_IDX\", outputCol=\"SERVER_TCP_FLAGS_VEC\"),\n",
    "    OneHotEncoder(inputCol=\"Attack_IDX\", outputCol=\"Attack_VEC\"),\n",
    "    OneHotEncoder(inputCol=\"Dataset_IDX\", outputCol=\"Dataset_VEC\")\n",
    "]\n",
    "\n",
    "# Combine stages into a pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Fit and transform the data\n",
    "df_encoded = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Display the resulting columns to confirm the encoding\n",
    "df_encoded.select(\"PROTOCOL_VEC\", \"TCP_FLAGS_VEC\", \"Attack_VEC\", \"Dataset_VEC\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6c636a-f7c2-4f59-ba25-d2a8b660d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                     |Label|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(454,[0,1,2,3,6,7,9,10,11,12,14,15,17,18,21,24,28,31,287,339,388,433,451],[65389.0,80.0,420.0,3.0,4293092.0,1875.0,64.0,64.0,140.0,140.0,140.0,140280.0,140.0,1.0,1120000.0,3.0,512.0,1.0,1.0,1.0,1.0,1.0,1.0])              |1    |\n",
      "|(454,[0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,22,23,24,28,31,288,339,390,433,451],[11154.0,80.0,280.0,2.0,40.0,1.0,4294499.0,453.0,64.0,64.0,140.0,40.0,40.0,140.0,280.0,40.0,320000.0,1.0,2.0,512.0,1.0,1.0,1.0,1.0,1.0,1.0])|1    |\n",
      "|(454,[0,1,2,3,4,5,11,12,13,14,15,16,21,22,23,28,31,288,339,390,431,453],[42062.0,1041.0,44.0,1.0,40.0,1.0,44.0,40.0,40.0,44.0,44.0,40.0,352000.0,320000.0,2.0,1024.0,1.0,1.0,1.0,1.0,1.0,1.0])                               |0    |\n",
      "|(454,[0,1,2,3,4,5,11,12,13,14,15,16,21,22,23,28,31,288,339,390,431,453],[46849.0,9110.0,44.0,1.0,40.0,1.0,44.0,40.0,40.0,44.0,44.0,40.0,352000.0,320000.0,2.0,1024.0,1.0,1.0,1.0,1.0,1.0,1.0])                               |0    |\n",
      "|(454,[0,1,2,3,4,5,11,12,13,14,15,16,21,22,23,28,31,288,339,390,431,453],[50360.0,1084.0,44.0,1.0,40.0,1.0,44.0,40.0,40.0,44.0,44.0,40.0,352000.0,320000.0,2.0,1024.0,1.0,1.0,1.0,1.0,1.0,1.0])                               |0    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the list of continuous and encoded feature columns\n",
    "continuous_features = [\n",
    "    \"L4_SRC_PORT\", \"L4_DST_PORT\", \"IN_BYTES\", \"IN_PKTS\", \"OUT_BYTES\", \n",
    "    \"OUT_PKTS\", \"FLOW_DURATION_MILLISECONDS\", \"DURATION_IN\", \"DURATION_OUT\", \n",
    "    \"MIN_TTL\", \"MAX_TTL\", \"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \n",
    "    \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"SRC_TO_DST_SECOND_BYTES\", \n",
    "    \"DST_TO_SRC_SECOND_BYTES\", \"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\", \n",
    "    \"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\", \"SRC_TO_DST_AVG_THROUGHPUT\", \n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\", \"NUM_PKTS_UP_TO_128_BYTES\", \"NUM_PKTS_128_TO_256_BYTES\", \n",
    "    \"NUM_PKTS_256_TO_512_BYTES\", \"NUM_PKTS_512_TO_1024_BYTES\", \"NUM_PKTS_1024_TO_1514_BYTES\", \n",
    "    \"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\", \"FTP_COMMAND_RET_CODE\"\n",
    "]\n",
    "encoded_features = [\"PROTOCOL_VEC\", \"TCP_FLAGS_VEC\", \"CLIENT_TCP_FLAGS_VEC\", \n",
    "                    \"SERVER_TCP_FLAGS_VEC\", \"Attack_VEC\", \"Dataset_VEC\"]\n",
    "\n",
    "# Assemble all features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=continuous_features + encoded_features,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Transform the data to add the 'features' column\n",
    "df_final = assembler.transform(df_encoded)\n",
    "\n",
    "# Select the features and label columns for the final dataset\n",
    "df_final = df_final.select(\"features\", \"Label\")\n",
    "\n",
    "# Show a sample of the final dataset\n",
    "df_final.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c301ccf9-74fe-430d-9a95-163dc21d6276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 53186564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>(102 + 1) / 103]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 22801412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assuming 'df_final' is a DataFrame already defined and available for splitting\n",
    "train_df, test_df = df_final.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Save the DataFrames to HDFS under the datasets directory\n",
    "train_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://localhost:54310/datasets/train_df\")\n",
    "test_df.write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://localhost:54310/datasets/test_df\")\n",
    "\n",
    "# Display the number of records in each dataset\n",
    "print(\"Training Dataset Count: \" + str(train_df.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_df.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f164c5-b672-4968-99b2-27ac4dec81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 12:20:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|Label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(454,[0,1,2,3,4,5...|    0|       0.0|\n",
      "|(454,[0,1,2,3,4,5...|    0|       0.0|\n",
      "|(454,[0,1,2,3,4,5...|    0|       0.0|\n",
      "|(454,[0,1,2,3,4,5...|    0|       0.0|\n",
      "|(454,[0,1,2,3,4,5...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark/spark-3.4.4-bin-hadoop3')  # Adjust to your specific Spark installation path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with adjusted configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomForestModel\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.instances\", \"3\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "    .config(\"spark.default.parallelism\", \"12\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# Load the training and testing data from HDFS\n",
    "train_df = spark.read.format(\"parquet\").load(\"hdfs://localhost:54310/datasets/train_df\")\n",
    "test_df = spark.read.format(\"parquet\").load(\"hdfs://localhost:54310/datasets/test_df\")\n",
    "\n",
    "# Initialize the RandomForest model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='Label', numTrees=10)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Show some prediction results\n",
    "predictions.select(\"features\", \"Label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ed38db-0e39-47e9-ab58-39952448df92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.997644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.978895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.978871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the model using BinaryClassificationEvaluator for ROC AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Label\")\n",
    "auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "print(\"Area under ROC = %g\" % auc)\n",
    "\n",
    "# Evaluate the model using MulticlassClassificationEvaluator for Accuracy and F1-Score\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"Label\", metricName=\"accuracy\")\n",
    "accuracy = multi_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "\n",
    "f1 = multi_evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "print(\"F1 Score = %g\" % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119f965-896f-47d9-a246-c0fe595af591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the training and testing data from HDFS\n",
    "# train_df = spark.read.format(\"parquet\").load(\"hdfs://localhost:54310/datasets/train_df\")\n",
    "# test_df = spark.read.format(\"parquet\").load(\"hdfs://localhost:54310/datasets/test_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6844a8-06b3-40fb-b96d-3ed434285304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 36:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 7274911.   274516.]\n",
      " [  206702. 15045283.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Convert DataFrame predictions to RDD to use with MulticlassMetrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"Label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "\n",
    "# Compute confusion matrix\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87580034-ce88-4e4c-8392-12a2141a070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the path in HDFS where you want to save the model\n",
    "model_path = \"hdfs://localhost:54310/datasets/randomforest_model\"\n",
    "\n",
    "# Save the trained RandomForest model\n",
    "rf_model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8287ea-6962-4c86-984b-26b30ae2af58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd195bd0-e151-4343-9ead-23ade4f29f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'confusion_matrix' is your numpy array from earlier\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# For example:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# confusion_matrix = np.array([[7274911, 274516], [206702, 15045283]])\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'confusion_matrix' is your numpy array from earlier\n",
    "# For example:\n",
    "# confusion_matrix = np.array([[7274911, 274516], [206702, 15045283]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))  # Set figure size for better visibility\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "\n",
    "ax.set_title('Confusion Matrix')  # Title for the heatmap\n",
    "ax.set_xlabel('Predicted Labels')  # Label for x-axis\n",
    "ax.set_ylabel('Actual Labels')  # Label for y-axis\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0078e5-1c21-4b57-a312-200ef1a74c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9789\n",
      "Precision: 0.9821\n",
      "Recall: 0.9864\n",
      "F1 Score: 0.9843\n"
     ]
    }
   ],
   "source": [
    "# Calculating additional metrics from the confusion matrix\n",
    "total = confusion_matrix.sum()\n",
    "accuracy = (confusion_matrix[0, 0] + confusion_matrix[1, 1]) / total\n",
    "precision = confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[0, 1])\n",
    "recall = confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[1, 0])\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ac33f2-c175-43e0-83ad-588500096b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 18:41:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark/spark-3.4.4-bin-hadoop3')  # Adjust to your specific Spark installation path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with adjusted configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyDetection\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.instances\", \"3\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "    .config(\"spark.default.parallelism\", \"12\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "hdfs_path = \"hdfs://localhost:54310/datasets/NF-UQ-NIDS-v2.csv\" \n",
    "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4873d7-e30a-4d3c-80ea-4c6fb21b2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IPV4_SRC_ADDR: string (nullable = true)\n",
      " |-- L4_SRC_PORT: integer (nullable = true)\n",
      " |-- IPV4_DST_ADDR: string (nullable = true)\n",
      " |-- L4_DST_PORT: integer (nullable = true)\n",
      " |-- PROTOCOL: integer (nullable = true)\n",
      " |-- L7_PROTO: double (nullable = true)\n",
      " |-- IN_BYTES: integer (nullable = true)\n",
      " |-- IN_PKTS: integer (nullable = true)\n",
      " |-- OUT_BYTES: integer (nullable = true)\n",
      " |-- OUT_PKTS: integer (nullable = true)\n",
      " |-- TCP_FLAGS: integer (nullable = true)\n",
      " |-- CLIENT_TCP_FLAGS: integer (nullable = true)\n",
      " |-- SERVER_TCP_FLAGS: integer (nullable = true)\n",
      " |-- FLOW_DURATION_MILLISECONDS: integer (nullable = true)\n",
      " |-- DURATION_IN: integer (nullable = true)\n",
      " |-- DURATION_OUT: integer (nullable = true)\n",
      " |-- MIN_TTL: integer (nullable = true)\n",
      " |-- MAX_TTL: integer (nullable = true)\n",
      " |-- LONGEST_FLOW_PKT: integer (nullable = true)\n",
      " |-- SHORTEST_FLOW_PKT: integer (nullable = true)\n",
      " |-- MIN_IP_PKT_LEN: integer (nullable = true)\n",
      " |-- MAX_IP_PKT_LEN: integer (nullable = true)\n",
      " |-- SRC_TO_DST_SECOND_BYTES: double (nullable = true)\n",
      " |-- DST_TO_SRC_SECOND_BYTES: double (nullable = true)\n",
      " |-- RETRANSMITTED_IN_BYTES: integer (nullable = true)\n",
      " |-- RETRANSMITTED_IN_PKTS: integer (nullable = true)\n",
      " |-- RETRANSMITTED_OUT_BYTES: integer (nullable = true)\n",
      " |-- RETRANSMITTED_OUT_PKTS: integer (nullable = true)\n",
      " |-- SRC_TO_DST_AVG_THROUGHPUT: long (nullable = true)\n",
      " |-- DST_TO_SRC_AVG_THROUGHPUT: long (nullable = true)\n",
      " |-- NUM_PKTS_UP_TO_128_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_128_TO_256_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_256_TO_512_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_512_TO_1024_BYTES: integer (nullable = true)\n",
      " |-- NUM_PKTS_1024_TO_1514_BYTES: integer (nullable = true)\n",
      " |-- TCP_WIN_MAX_IN: integer (nullable = true)\n",
      " |-- TCP_WIN_MAX_OUT: integer (nullable = true)\n",
      " |-- ICMP_TYPE: integer (nullable = true)\n",
      " |-- ICMP_IPV4_TYPE: integer (nullable = true)\n",
      " |-- DNS_QUERY_ID: integer (nullable = true)\n",
      " |-- DNS_QUERY_TYPE: integer (nullable = true)\n",
      " |-- DNS_TTL_ANSWER: long (nullable = true)\n",
      " |-- FTP_COMMAND_RET_CODE: double (nullable = true)\n",
      " |-- Label: integer (nullable = true)\n",
      " |-- Attack: string (nullable = true)\n",
      " |-- Dataset: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema to verify available columns\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18cefab-7f3e-41b7-a503-6a35376869e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------------+--------+---------+-------+--------+-----+\n",
      "|  IPV4_SRC_ADDR|IPV4_DST_ADDR|FLOW_DURATION_MILLISECONDS|IN_BYTES|OUT_BYTES|IN_PKTS|OUT_PKTS|Label|\n",
      "+---------------+-------------+--------------------------+--------+---------+-------+--------+-----+\n",
      "|192.168.100.148|192.168.100.7|                   4293092|     420|        0|      3|       0|    1|\n",
      "|192.168.100.148|192.168.100.5|                   4294499|     280|       40|      2|       1|    1|\n",
      "|   192.168.1.31| 192.168.1.79|                         0|      44|       40|      1|       1|    0|\n",
      "|   192.168.1.34| 192.168.1.79|                         0|      44|       40|      1|       1|    0|\n",
      "|   192.168.1.30|192.168.1.152|                         0|      44|       40|      1|       1|    0|\n",
      "+---------------+-------------+--------------------------+--------+---------+-------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the required columns for temporal analysis and anomaly detection\n",
    "df_anomaly = df.select(\n",
    "    \"IPV4_SRC_ADDR\", \n",
    "    \"IPV4_DST_ADDR\", \n",
    "    \"FLOW_DURATION_MILLISECONDS\", \n",
    "    \"IN_BYTES\", \n",
    "    \"OUT_BYTES\", \n",
    "    \"IN_PKTS\", \n",
    "    \"OUT_PKTS\", \n",
    "    \"Label\"\n",
    ")\n",
    "\n",
    "# Verify the contents of df_anomaly\n",
    "df_anomaly.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5266736d-e290-40e4-95f6-53abd57dcefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the optimized DataFrame in HDFS for future use\n",
    "df_anomaly.write.parquet(\"hdfs://localhost:54310/datasets/anomaly_detection_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80370d2-f38f-4c9f-93e4-ca5e26e9b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/11 18:56:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark/spark-3.4.4-bin-hadoop3')  # Adjust to your specific Spark installation path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with adjusted configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyDetection\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\")\\\n",
    "    .config(\"spark.executor.cores\", \"1\")\\\n",
    "    .config(\"spark.executor.instances\", \"3\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "    .config(\"spark.default.parallelism\", \"12\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "df_anomaly = spark.read.parquet(\"hdfs://localhost:54310/datasets/anomaly_detection_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97e804f-e0f9-4054-959d-cb15b2d45201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------------------+-------------------+\n",
      "|IPV4_SRC_ADDR|IPV4_DST_ADDR|FLOW_DURATION_MILLISECONDS|synthetic_timestamp|\n",
      "+-------------+-------------+--------------------------+-------------------+\n",
      "|0.109.229.230|192.168.1.152|                         0|                  0|\n",
      "|    0.14.2.72|192.168.1.194|                         0|                  0|\n",
      "|0.163.152.173|192.168.1.190|                         0|                  0|\n",
      "|   1.0.126.42| 172.31.67.29|                         0|                  0|\n",
      "|   1.0.176.98| 172.31.67.96|                         0|                  0|\n",
      "+-------------+-------------+--------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window specification partitioned by source-destination, without ordering by a specific column\n",
    "window_spec = Window.partitionBy(\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\").orderBy(\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\")\n",
    "\n",
    "# Generate synthetic timestamps by incrementing within each partition\n",
    "df_anomaly = df_anomaly.withColumn(\"synthetic_timestamp\", row_number().over(window_spec) - 1)\n",
    "\n",
    "# Display the results to check if timestamps are generated as expected\n",
    "df_anomaly.select(\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", \"FLOW_DURATION_MILLISECONDS\", \"synthetic_timestamp\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b77a275-4bb8-4d4c-a04a-aff80ff0cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/11 19:17:13 ERROR Executor: Exception in task 6.0 in stage 35.0 (TID 162)]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$1682/0x0000000840cfd040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:271)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:271)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda$1691/0x0000000840d03840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:249)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:271)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$1690/0x0000000840d03040.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1688/0x0000000840d01440.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1495)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
      "24/11/11 19:17:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 35.0 (TID 162),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$1682/0x0000000840cfd040.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1849)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:708)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:271)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:271)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda$1691/0x0000000840d03840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:249)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:271)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$1690/0x0000000840d03040.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda$1688/0x0000000840d01440.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1495)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1450)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1169)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:345)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)\n",
      "24/11/11 19:17:13 ERROR TaskSetManager: Task 6 in stage 35.0 failed 1 times; aborting job\n",
      "24/11/11 19:17:13 ERROR Utils: Uncaught exception in thread task-result-getter-1\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1171)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1115)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1115)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1255)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Exception in thread \"task-result-getter-1\" java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1171)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1115)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1115)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1255)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/11 19:17:13 ERROR ChunkFetchRequestHandler: Error sending result ChunkFetchSuccess[streamChunkId=StreamChunkId[streamId=1907653754004,chunkIndex=0],buffer=org.apache.spark.storage.BlockManagerManagedBuffer@41fc2503] to /10.154.0.2:41640; closing connection\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel.close(ChannelPromise)(Unknown Source)\n",
      "24/11/11 19:17:13 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from hadoop.europe-west2-b.c.acoustic-spot-438021-c6.internal/10.154.0.2:34805 is closed\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/pyspark/sql/dataframe.py:1218\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1218\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o248.collectToPython",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Collect the relevant columns into a Pandas DataFrame for processing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_anomaly_pd \u001b[38;5;241m=\u001b[39m \u001b[43mdf_anomaly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIPV4_SRC_ADDR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIPV4_DST_ADDR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOW_DURATION_MILLISECONDS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/pyspark/sql/dataframe.py:1218\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1218\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/pyspark/traceback_utils.py:81\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     79\u001b[0m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/spark-3.4.4-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Collect the relevant columns into a Pandas DataFrame for processing\n",
    "df_anomaly_pd = df_anomaly.select(\"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\", \"FLOW_DURATION_MILLISECONDS\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe6c117-7790-4e38-812e-5f65a040c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|FLOW_DURATION_MILLISECONDS|\n",
      "+--------------------------+\n",
      "|                   4293092|\n",
      "|                   4294655|\n",
      "|                   4293436|\n",
      "|                   4293451|\n",
      "|                   4293572|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero duration count: 35017386\n",
      "Non-zero duration count: 40970590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for any non-zero values in FLOW_DURATION_MILLISECONDS\n",
    "df_anomaly.select(\"FLOW_DURATION_MILLISECONDS\").distinct().show(5)\n",
    "\n",
    "# Alternatively, count the rows where FLOW_DURATION_MILLISECONDS is zero and non-zero\n",
    "zero_count = df_anomaly.filter(df_anomaly[\"FLOW_DURATION_MILLISECONDS\"] == 0).count()\n",
    "non_zero_count = df_anomaly.filter(df_anomaly[\"FLOW_DURATION_MILLISECONDS\"] > 0).count()\n",
    "\n",
    "print(f\"Zero duration count: {zero_count}\")\n",
    "print(f\"Non-zero duration count: {non_zero_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
